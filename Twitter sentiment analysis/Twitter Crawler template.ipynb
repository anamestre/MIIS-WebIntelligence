{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the twitter library in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (1.7.1)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.9.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install library for JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplejson in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install simplejson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install sentiment analysis library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in /Users/albertlleo/opt/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# Whatever library you use\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming tweets and perform some data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up and running a streaming crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting code\n",
      "<tweepy.auth.OAuthHandler object at 0x10ab27f50>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'twitter_stream = Stream(auth, MyListener(\\'gobierno2.json\\'))\\ntwitter_stream.filter(track=[\"gobierno\", \"elecciones\"], languages=[\"es\"]) # Add your keywords and other filters\\n\\ntwitter_stream = Stream(auth, MyListener(\\'pp.json\\'))\\ntwitter_stream.filter(track=[\"pp\"], languages=[\"es\"]) # Add your keywords and other filters\\n\\ntwitter_stream = Stream(auth, MyListener(\\'psoe.json\\'))\\ntwitter_stream.filter(track=[\"psoe\"], languages=[\"es\"]) # Add your keywords and other filters\\n\\ntwitter_stream = Stream(auth, MyListener(\\'cat.json\\'))\\ntwitter_stream.filter(track=[\"cataluña\", \"catalunya\"], languages=[\"es\"]) # Add your keywords and other filters\\n\\ntwitter_stream = Stream(auth, MyListener(\\'vox.json\\'))\\ntwitter_stream.filter(track=[\"vox\"], languages=[\"es\"]) # Add your keywords and other filters\\n\\ntwitter_stream = Stream(auth, MyListener(\\'ciudadanos.json\\'))\\ntwitter_stream.filter(track=[\"ciudadanos\", \"ciutadans\"], languages=[\"es\"]) # Add your keywords and other filters\\n\\ntwitter_stream = Stream(auth, MyListener(\\'podemos.json\\'))\\ntwitter_stream.filter(track=[\"podemos\"], languages=[\"es\"]) # Add your keywords and other filters\\n\\nprint(\\'_______ End _______\\')'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import simplejson as json\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "from textblob import TextBlob\n",
    " \n",
    "#Complete with your keys \n",
    "\n",
    "consumer_key = 'aCqrloIHCuvw8rX8sON8B551T'\n",
    "consumer_secret = 'CkkLBIClzyC2w00oHOateou4wxJ2IberPKXWR8WpQEi0T5Gh2s'\n",
    "access_token = '173960222-v34xiH0gEFqXCoVvpnh3JgMp8xwVukAX61rqazBc'\n",
    "access_secret = '9ey7wPBUkPJdu2ZxaBoOUr43dmFCj2Fql1MxBCPKEoVFg'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "print(\"Starting code\")\n",
    "print(auth)\n",
    " \n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "    def __init__(self, filename, api=None):\n",
    "        super(StreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.filename = filename\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(self.filename, 'a') as f:\n",
    "                #print(\"test\")\n",
    "                if json.loads(data).get('place'):\n",
    "                    print(json.loads(data)['place']['country'])\n",
    "                    if json.loads(data)['place']['country'].lower() in ['spain', 'españa', 'espana']:\n",
    "                        f.write(data) # This will store the whole JSON data in the file, you can perform some JSON filters\n",
    "                        twitter_text = json.loads(data)['text'] # You can also print your tweets here\n",
    "                        print(twitter_text)\n",
    "                        self.num_tweets += 1\n",
    "\n",
    "                # Just to limit the number of tweets collected to check the \n",
    "                # program at the beginning, then increase the limit\n",
    "                if self.num_tweets < 100:\n",
    "                    return True\n",
    "                else:\n",
    "                    print('______________ END ', self.filename)\n",
    "                    return False\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    "\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print('Error :', status.place)\n",
    "        return False\n",
    "    \n",
    "\"\"\"twitter_stream = Stream(auth, MyListener('gobierno2.json'))\n",
    "twitter_stream.filter(track=[\"gobierno\", \"elecciones\"], languages=[\"es\"]) # Add your keywords and other filters\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener('pp.json'))\n",
    "twitter_stream.filter(track=[\"pp\"], languages=[\"es\"]) # Add your keywords and other filters\n",
    "\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener('psoe.json'))\n",
    "twitter_stream.filter(track=[\"psoe\"], languages=[\"es\"]) # Add your keywords and other filters\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener('cat.json'))\n",
    "twitter_stream.filter(track=[\"cataluña\", \"catalunya\"], languages=[\"es\"]) # Add your keywords and other filters\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener('vox.json'))\n",
    "twitter_stream.filter(track=[\"vox\"], languages=[\"es\"]) # Add your keywords and other filters\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener('ciudadanos.json'))\n",
    "twitter_stream.filter(track=[\"ciudadanos\", \"ciutadans\"], languages=[\"es\"]) # Add your keywords and other filters\n",
    "\"\"\"\n",
    "twitter_stream = Stream(auth, MyListener('podemos.json'))\n",
    "twitter_stream.filter(track=[\"podemos\"], languages=[\"es\"]) # Add your keywords and other filters\n",
    "\n",
    "print('_______ End _______')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the JSON data in a CSV for analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "\n",
    "# Create the CSV file\n",
    "#files = [\"gobierno\", \"cat\", \"podemos\", \"pp\", \"vox\", \"ciudadanos\", \"psoe\"]\n",
    "files = [\"pp\"]\n",
    "\n",
    "for file in files:\n",
    "    with open (file + \".csv\", 'w', encoding ='utf-8') as csv:\n",
    "        # Write the title of the columns (features) that you want to store in the CSV file\n",
    "        csv.write('id,'+'created_at,'+'text'+'\\n')\n",
    "\n",
    "        # Copy the data from the JSON file\n",
    "        with open(file + \".json\", 'r', encoding ='utf-8') as jsonfile:\n",
    "            for tweet in jsonfile: \n",
    "                data = json.loads(tweet)\n",
    "\n",
    "                # The int values should be converted to strings\n",
    "                csv.write(str(data['id'])+',')\n",
    "                csv.write(str(data['created_at'])+',') \n",
    "                csv.write((str(data['text']).replace('\\n', \"\").replace(',', \"\"))) \n",
    "                #csv.write(str(data['place']['country'])) \n",
    "                csv.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the previous CSV into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1219242646469775360</td>\n",
       "      <td>Mon Jan 20 12:57:54 +0000 2020</td>\n",
       "      <td>El #PinParental no se aplica a asignaturas cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219243127132868608</td>\n",
       "      <td>Mon Jan 20 12:59:48 +0000 2020</td>\n",
       "      <td>@Santi_ABASCAL Por fin un partido que defiende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219243480255467520</td>\n",
       "      <td>Mon Jan 20 13:01:12 +0000 2020</td>\n",
       "      <td>La ocurrencia de Vox del veto parental le esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219243557908877314</td>\n",
       "      <td>Mon Jan 20 13:01:31 +0000 2020</td>\n",
       "      <td>@meneses @nostraeuropa @diazvillanueva Diazvil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219244207048658944</td>\n",
       "      <td>Mon Jan 20 13:04:06 +0000 2020</td>\n",
       "      <td>Estoy contigo todos unidos derrotaremos al ene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219312809156235264</td>\n",
       "      <td>Mon Jan 20 17:36:42 +0000 2020</td>\n",
       "      <td>Vox se rie del PP...o el PP se deja ser un haz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219313321305952256</td>\n",
       "      <td>Mon Jan 20 17:38:44 +0000 2020</td>\n",
       "      <td>@pmarzo1 @pdiaz222f @eldiarioe ¿Torear?. Parec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219313733471744000</td>\n",
       "      <td>Mon Jan 20 17:40:22 +0000 2020</td>\n",
       "      <td>@SimnMuoz @MasPais_Es @ierrejon Los maleducado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219313981644574721</td>\n",
       "      <td>Mon Jan 20 17:41:21 +0000 2020</td>\n",
       "      <td>@Francis16893780 @InesArrimadas El PP y Ciudad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219314237564166146</td>\n",
       "      <td>Mon Jan 20 17:42:22 +0000 2020</td>\n",
       "      <td>@InesArrimadas Salgamos del posicionam de Vox ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         created_at  \\\n",
       "id                                                    \n",
       "1219242646469775360  Mon Jan 20 12:57:54 +0000 2020   \n",
       "1219243127132868608  Mon Jan 20 12:59:48 +0000 2020   \n",
       "1219243480255467520  Mon Jan 20 13:01:12 +0000 2020   \n",
       "1219243557908877314  Mon Jan 20 13:01:31 +0000 2020   \n",
       "1219244207048658944  Mon Jan 20 13:04:06 +0000 2020   \n",
       "...                                             ...   \n",
       "1219312809156235264  Mon Jan 20 17:36:42 +0000 2020   \n",
       "1219313321305952256  Mon Jan 20 17:38:44 +0000 2020   \n",
       "1219313733471744000  Mon Jan 20 17:40:22 +0000 2020   \n",
       "1219313981644574721  Mon Jan 20 17:41:21 +0000 2020   \n",
       "1219314237564166146  Mon Jan 20 17:42:22 +0000 2020   \n",
       "\n",
       "                                                                  text  \n",
       "id                                                                      \n",
       "1219242646469775360  El #PinParental no se aplica a asignaturas cur...  \n",
       "1219243127132868608  @Santi_ABASCAL Por fin un partido que defiende...  \n",
       "1219243480255467520  La ocurrencia de Vox del veto parental le esta...  \n",
       "1219243557908877314  @meneses @nostraeuropa @diazvillanueva Diazvil...  \n",
       "1219244207048658944  Estoy contigo todos unidos derrotaremos al ene...  \n",
       "...                                                                ...  \n",
       "1219312809156235264  Vox se rie del PP...o el PP se deja ser un haz...  \n",
       "1219313321305952256  @pmarzo1 @pdiaz222f @eldiarioe ¿Torear?. Parec...  \n",
       "1219313733471744000  @SimnMuoz @MasPais_Es @ierrejon Los maleducado...  \n",
       "1219313981644574721  @Francis16893780 @InesArrimadas El PP y Ciudad...  \n",
       "1219314237564166146  @InesArrimadas Salgamos del posicionam de Vox ...  \n",
       "\n",
       "[88 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tweets_all = pd.read_csv('pp.csv', index_col = 0, encoding='utf-8')\n",
    "tweets_all.head(100)\n",
    "#count=0\n",
    "#for tweet in tweets_all:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the polarity of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gabi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets percentage: 0.0 %\n",
      "Negative tweets percentage: 1.1363636363636365 %\n",
      "Neutral tweets percentage: 98.86363636363636 % \n",
      "\n",
      "\n",
      "Positive tweets:\n",
      "\n",
      "\n",
      "Negative tweets:\n",
      "@beaveil @BertoEsquer Ojo y en base a unos valores que ordena la LOMCE que una ley del PP!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import re, tweepy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "def clean_tweet(tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        #return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('spanish')) + r')\\b\\s*')\n",
    "        return pattern.sub('', tweet)\n",
    "\n",
    "\n",
    "def get_tweet_sentiment(tweet): \n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet \n",
    "    using textblob's sentiment method \n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'                               \n",
    "\n",
    "def featched_tweets(tweets_):\n",
    "    try:\n",
    "        tweets_clean = []\n",
    "        for tweet in tweets_:\n",
    "            # empty dictionary to store required params of a tweet \n",
    "            parsed_tweet = {} \n",
    "\n",
    "            # saving text of tweet \n",
    "            parsed_tweet['text'] = tweet \n",
    "            # saving sentiment of tweet \n",
    "            parsed_tweet['sentiment'] = get_tweet_sentiment(tweet)\n",
    "\n",
    "            # appending parsed tweet to tweets list \n",
    "            tweets_clean.append(parsed_tweet) \n",
    "\n",
    "        # return parsed tweets \n",
    "        return tweets_clean\n",
    "        \n",
    "    except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e))\n",
    "                               \n",
    "def main(): \n",
    "    tweets = featched_tweets(tweets_all[\"text\"])\n",
    "    # picking positive tweets from tweets found before\n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    # percentage of positive tweets \n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "    # picking negative tweets from tweets \n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "    # percentage of negative tweets \n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "    # percentage of neutral tweets \n",
    "    neutral = list(filter(lambda l: l not in ntweets and l not in ptweets, tweets))\n",
    "    print(\"Neutral tweets percentage: {} % \".format(100*len(neutral)/len(tweets)))\n",
    "  \n",
    "    # printing first 5 positive tweets \n",
    "    print(\"\\n\\nPositive tweets:\") \n",
    "    for tweet in ptweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "    # printing first 5 negative tweets \n",
    "    print(\"\\n\\nNegative tweets:\") \n",
    "    for tweet in ntweets[:10]: \n",
    "        print(tweet['text']) \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()                                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/af/55c7f2aa0997147943b474a74bab8deb17e7cf935b9abb8798d724c57721/wordcloud-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (361kB)\n",
      "\u001b[K     |████████████████████████████████| 368kB 901kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from wordcloud) (1.17.2)\n",
      "Requirement already satisfied: pillow in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from wordcloud) (6.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from wordcloud) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: six in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/gabi/Installations/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-8ce071d49c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tweets = map(clean_tweet, tweets_all[\"text\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-8ce071d49c01>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tweets = map(clean_tweet, tweets_all[\"text\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-031ccffbf822>\u001b[0m in \u001b[0;36mclean_tweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     11\u001b[0m         '''\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\b('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mr'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mr')\\b\\s*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tweets = map(clean_tweet, tweets_all[\"text\"])\n",
    "tweets = [clean_tweet(tweet) for tweet in tweets_all['text']]\n",
    "\n",
    "text = \" \".join(review for review in tweets)\n",
    "print(\"There are {} words in the combination of all review. \".format(len(text)))\n",
    "\n",
    "#create stopword list\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"http\",\"https\", \"co\"])\n",
    "\n",
    "# generate wordcloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,background_color=\"white\").generate(text)\n",
    "\n",
    "#Display generated image:\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \" \".join(review for review in tweets.text)\n",
    "print(\"There are {} words in the combination of all review. \".format(len(text)))\n",
    "\n",
    "#create stopword list\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"http\",\"https\", \"co\"])\n",
    "\n",
    "# generate wordcloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,background_color=\"white\").generate(text)\n",
    "\n",
    "#Display generated image:\n",
    "\n",
    "plt.figure(fgisize=(10,15))\n",
    "plt.imshow(worldcloud, interplotation'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your own analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
